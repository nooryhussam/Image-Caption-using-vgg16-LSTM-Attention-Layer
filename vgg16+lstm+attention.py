# -*- coding: utf-8 -*-
"""Vgg16+LSTM+Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PnKCuzCvSfpAtsSYU4-Wvbo2fZnmB2YR

### **Connect Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""### **Load Dataset**"""

from google.colab import files
files.upload()

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d adityajn105/flickr8k
!unzip flickr8k.zip -d flickr8k

"""### **Imports**"""

import os
import re
import pickle
import numpy as np
from tqdm.notebook import tqdm
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
from tensorflow.keras import layers
from tensorflow.keras.layers import Layer
from tensorflow.keras import backend as K

"""### **Preprocessing**"""

BASE_DIR = '/content/drive/MyDrive/flickr8k'
WORKING_DIR = '/content'

def clean(mapping):
    for key, captions in mapping.items():
        for i in range(len(captions)):

            caption = captions[i].lower()

            caption = re.sub(r'[^a-zA-Z]', ' ', caption)

            caption = re.sub(r'\s+', ' ', caption)

            caption = 'startseq ' + " ".join([word for word in caption.split() if len(word) > 1]) + ' endseq'

            captions[i] = caption

with open(os.path.join(BASE_DIR, '/content/drive/MyDrive/flickr8k/captions.txt'), 'r') as f:
    next(f)
    captions_doc = f.read()

# create mapping of image to captions
mapping = {}

for line in tqdm(captions_doc.split('\n')):

    tokens = line.split(',')
    if len(line) < 2:
        continue
    image_id, caption = tokens[0], tokens[1:]

    image_id = image_id.split('.')[0]

    caption = " ".join(caption)

    if image_id not in mapping:
        mapping[image_id] = []

    mapping[image_id].append(caption)

mapping['1000268201_693b08cb0e']

clean(mapping)
mapping['1000268201_693b08cb0e']

all_captions = []
for key in mapping:
    for caption in mapping[key]:
        all_captions.append(caption)

len(all_captions)

all_captions[:10]

"""### **Tokenization**"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1

print(vocab_size)

with open('/content/drive/MyDrive/tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

max_length = max(len(caption.split()) for caption in all_captions)
max_length

"""### **Feature Extract**"""

model = VGG16() #load model
model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
print(model.summary())

features = {}

directory = os.path.join(BASE_DIR, 'Images')

for img_name in tqdm(os.listdir(directory)):

    img_path = os.path.join(directory, img_name)

    image = load_img(img_path, target_size=(224, 224))

    image = img_to_array(image)

    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))

    image = preprocess_input(image)

    feature = model.predict(image, verbose=0)

    image_id = img_name.split('.')[0]

    features[image_id] = feature

"""### **Load and Store Features**"""

# store features in pickle
pickle.dump(features, open(os.path.join(WORKING_DIR, '/content/drive/MyDrive/features.pkl'), 'wb'))

# load features from pickle
with open(os.path.join(WORKING_DIR, '/content/drive/MyDrive/features.pkl'), 'rb') as f:
    features = pickle.load(f)

"""### **Split Train and Test**"""

image_ids = list(mapping.keys())
split = int(len(image_ids) * 0.90)
train = image_ids[:split]
test = image_ids[split:]

"""### **Data** **Generator**"""

def data_generator(image_ids, captions_mapping, image_features_dict, tokenizer, max_length, vocab_size, batch_size):
    while True:
        image_features, input_captions, target_words = [], [], []
        count = 0

        for image_id in image_ids:
            if image_id not in image_features_dict:
                print(f"Image {image_id} has no data, skipping it and continuing.")
                continue

            captions = captions_mapping[image_id]

            for caption in captions:
                caption_sequence = tokenizer.texts_to_sequences([caption])[0]

                for i in range(1, len(caption_sequence)):
                    partial_caption = caption_sequence[:i]
                    next_word = caption_sequence[i]

                    partial_caption = pad_sequences([partial_caption], maxlen=max_length, padding='post')[0]
                    next_word = to_categorical(next_word, num_classes=vocab_size)

                    image_features.append(image_features_dict[image_id][0])
                    input_captions.append(partial_caption)
                    target_words.append(next_word)

            count += 1

            if count == batch_size:

                yield {"image_input": np.array(image_features), "text_input": np.array(input_captions)}, np.array(target_words)
                image_features, input_captions, target_words = [], [], []
                count = 0

"""### **Model Creation**"""

class Attention(Layer):
    def __init__(self, attention_dim, **kwargs):
        super(Attention, self).__init__(**kwargs)
        self.attention_dim = attention_dim

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], self.attention_dim),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(shape=(self.attention_dim,),
                                 initializer='zeros',
                                 trainable=True)
        self.u = self.add_weight(shape=(self.attention_dim, 1),
                                 initializer='glorot_uniform',
                                 trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, inputs):
        u_it = K.tanh(K.dot(inputs, self.W) + self.b)       # shape: (batch_size, time_steps, attention_dim)
        a_it = K.softmax(K.dot(u_it, self.u), axis=1)       # shape: (batch_size, time_steps, 1)
        output = inputs * a_it                              # shape: (batch_size, time_steps, features)
        return K.sum(output, axis=1)                        # shape: (batch_size, features)

# Image feature layers
from tensorflow.keras.layers import BatchNormalization

inputs1 = Input(shape=(4096,), name="image_input")
fe1 = Dropout(0.4)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
fe2 = BatchNormalization()(fe2)


# Sequence feature layers (Text inputs)
inputs2 = Input(shape=(max_length,), name="text_input")
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.4)(se1)
se3 = LSTM(256, return_sequences=True)(se2)

# Attention Layer
attention = Attention(attention_dim=256)(se3)

# Decoder part (Combining Image and Attention-based LSTM features)
decoder1 = add([fe2, attention])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

# Model definition
model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""### **Train Model**"""

epochs = 90
batch_size = 64
steps_per_epoch = len(train) // batch_size
val_steps = len(test) // batch_size

train_generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)
val_generator = data_generator(test, mapping, features, tokenizer, max_length, vocab_size, batch_size)

model.fit(train_generator,
          epochs=epochs,
          steps_per_epoch=steps_per_epoch,
          validation_data=val_generator,
          validation_steps=val_steps,
          verbose=1)

model.save('/content/drive/MyDrive/train_model_90.keras')

from tensorflow.keras.models import load_model

model = load_model('/content/drive/MyDrive/train_model_90.keras', custom_objects={'Attention': Attention})

def idx_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
          return word
    return None

def predict_caption(model, image, tokenizer, max_length):
    in_text = 'startseq'

    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')
        yhat = model.predict([image.reshape(1, 4096), sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = idx_to_word(yhat, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break

    final_caption = in_text.split()[1:-1]
    return ' '.join(final_caption)

from nltk.translate.bleu_score import corpus_bleu

# validate with test data
actual, predicted = list(), list()

for key in tqdm(test):
    # get actual captions
    captions = mapping[key]
    actual_captions = [caption.split() for caption in captions]

    # predict caption for image
    y_pred = predict_caption(model, features[key], tokenizer, max_length)
    y_pred = y_pred.split()

    actual.append(actual_captions)
    predicted.append(y_pred)

print("BLEU-1: %f" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))
print("BLEU-2: %f" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))
print("BLEU-3: %f" % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))
print("BLEU-4: %f" % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))

"""## **Test**"""

import random
from PIL import Image
import matplotlib.pyplot as plt

def test_random_image(model, features, tokenizer, max_length, image_dir):
    image_ids = list(features.keys())
    image_id = random.choice(image_ids)

    image_path = os.path.join(image_dir, 'Images', image_id + '.jpg')
    image = Image.open(image_path)
    plt.imshow(image)
    plt.axis('off')

    image_feature = features[image_id]
    caption = predict_caption(model, image_feature, tokenizer, max_length)
    print(caption)

image_directory = BASE_DIR
test_random_image(model, features, tokenizer, max_length, image_directory)

from google.colab import files
files.upload()